{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "print wikipedia.summary(\"アルコ&ピースのオールナイトニッポンシリーズ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"contents.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.index:\n",
    "    print wikipedia.summary(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html \n",
    "import urllib3\n",
    "import json\n",
    "import requests\n",
    "\n",
    "urllib3.disable_warnings()\n",
    "http = urllib3.PoolManager()\n",
    "# r = http.request('GET', 'http://ja.wikipedia.org/w/api.php?format=xml&action=query&prop=revisions&titles=朝井リョウ%26加藤千恵のオールナイトニッポン0(ZERO)&rvprop=content')\n",
    "r = http.request('GET', 'http://ja.wikipedia.org/w/api.php?format=json&action=query&prop=revisions&titles=朝井リョウ%26加藤千恵のオールナイトニッポン0(ZERO)&rvprop=content')\n",
    "print r.data\n",
    "\n",
    "#print data\n",
    "# html = lxml.html.fromstring(data)\n",
    "# title = html.xpath('query/pages/page/@title')[0];\n",
    "# print(html)\n",
    "# print(r.data)\n",
    "# print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "url =  'http://ja.wikipedia.org/w/api.php?format=json&action=query&prop=revisions&titles=朝井リョウ%26加藤千恵のオールナイトニッポン0(ZERO)&rvprop=content'\n",
    "# url =  'http://ja.wikipedia.org/w/api.php?format=json&action=query&prop=revisions&titles=ウレロ☆シリーズ&rvprop=content'\n",
    "# url =  'http://en.wikipedia.org/w/api.php?format=json&action=query&prop=revisions&titles==Emma_Watson&rvprop=content'\n",
    "# url =' http://ja.wikipedia.org/w/api.php?format=json&action=query&prop=revisions&titles=Main%20Page&rvprop=content'\n",
    "# url = 'https://en.wikipedia.org/w/api.php?action=query&titles=Main%20Page&prop=revisions&rvprop=content&format=json&rvprop=content'\n",
    "# r = requests.get(url)\n",
    "# data = r.json()\n",
    "# print data # print json.dumps(data, indent=4)\n",
    "resp = requests.get(url).json()\n",
    "# print resp.keys(), resp.values()\n",
    "page_one = next(iter(resp['query']['pages'].values()))\n",
    "#print page_one\n",
    "# print len(resp['query']['pages'])\n",
    "# print page_one.keys()\n",
    "# print page_one['title']\n",
    "revisions = page_one.get('revisions', [])\n",
    "#print revisions\n",
    "# html = next(iter(revisions[0].values()))\n",
    "\n",
    "print revisions[0]['*']\n",
    "# print revisions[0][u'contentmodel']\n",
    "# print revisions[0][u'contentformat']\n",
    "\n",
    "# now parse the html \n",
    "\n",
    "\n",
    "# r = http.request('GET', url)\n",
    "# print r.data\n",
    "# html = lxml.html.fromstring(r.data.decode('utf-8'))\n",
    "# title = html.xpath('query/pages/page/@title')[0];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the way to get infobox from wikipedia with pandas\n",
    "import pandas\n",
    "# fetched_dataframes = pandas.io.html.read_html('https://ja.wikipedia.org/wiki/%E6%9C%9D%E4%BA%95%E3%83%AA%E3%83%A7%E3%82%A6%26%E5%8A%A0%E8%97%A4%E5%8D%83%E6%81%B5%E3%81%AE%E3%82%AA%E3%83%BC%E3%83%AB%E3%83%8A%E3%82%A4%E3%83%88%E3%83%8B%E3%83%83%E3%83%9D%E3%83%B30(ZERO)')\n",
    "# url = 'https://ja.wikipedia.org/wiki/朝井リョウ%26加藤千恵のオールナイトニッポン0(ZERO)'\n",
    "url ='https://ja.wikipedia.org/wiki/%E3%82%A6%E3%83%AC%E3%83%AD%E2%98%86%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA'\n",
    "infobox = pandas.read_html(url, match=u'ジャンル')\n",
    "for i in infobox:\n",
    "    print infobox[0].describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usage of wikipedia package\n",
    "import wikipedia\n",
    "wikipedia.set_lang('ja')\n",
    "\n",
    "# print wikipedia.summary('朝井リョウと加藤千恵のオールナイトニッポン0(ZERO)')\n",
    "# p = wikipedia.page('朝井リョウと加藤千恵のオールナイトニッポン0(ZERO)')\n",
    "p = wikipedia.page('ウレロ☆シリーズ')\n",
    "\n",
    "# print p.content\n",
    "# print p.categories\n",
    "# print dir(p)\n",
    "print 'カテゴリー'\n",
    "for v in p.categories:\n",
    "    print ' ', v\n",
    "\n",
    "#print p.content\n",
    "#print p.html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser, object):\n",
    "    def __init__(self):\n",
    "        super(MyHTMLParser, self).__init__()\n",
    "        self._find_headline = False\n",
    "        self._find_staff = False\n",
    "#         self._uli_count= 0\n",
    "        self._li_count= 0\n",
    "        self._dd_count= 0\n",
    "        self._bracket_count = 0\n",
    "        self._find_ref = False\n",
    "#         self._find_table = False\n",
    "#         self._find_colon = False\n",
    "        \n",
    "        self._index = ''\n",
    "        self._data_temp = ''\n",
    "        self._value = []\n",
    "        self._value_set = set()\n",
    "        self._staff_dict = dict()\n",
    "        \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if self._find_staff:\n",
    "#             print(\"Start tag:\", tag)\n",
    "            if tag == u'a':\n",
    "                self._link = True\n",
    "            if tag == u'li':\n",
    "                self._li_count +=1\n",
    "                self._data_temp = ''\n",
    "            if tag == u'dd':\n",
    "                self._dd_count += 1\n",
    "#             if tag ==\n",
    "       \n",
    "        for attr in attrs:\n",
    "#             if self._find_staff:\n",
    "#                 print(\"     attr:\", attr)\n",
    "            if 'mw-headline' in attr:\n",
    "                self._find_headline = True\n",
    "                self._find_staff = False\n",
    "            if 'reference' in attr:\n",
    "                self._find_ref = True\n",
    "\n",
    "            \n",
    "    def handle_endtag(self, tag):\n",
    "        if self._find_staff: \n",
    "#             print(\"End tag  :\", tag)\n",
    "#             if tag == u'uli':\n",
    "#                 self._uli_count -= 1\n",
    "            if tag == u'dd':\n",
    "                self._dd_count -= 1\n",
    "                \n",
    "                datas = re.split(u'[：-]', self._data_temp)\n",
    "                self._index = datas[0]\n",
    "                self._value.extend(re.split(u'[,、・/／→]', datas[1])) #separate multiple staffs\n",
    "                for i, v in enumerate(self._value): #remove () and ※\n",
    "                    self._value[i]  = re.split(u'[（(※]', v)[0]\n",
    "                self._staff_dict[self._index] = self._value                \n",
    "\n",
    "                self._value = []\n",
    "\n",
    "            if tag == u'li':\n",
    "                self._li_count -= 1\n",
    "#                 self._find_colon = False\n",
    "                datas = re.split(u'[：-]', self._data_temp)\n",
    "                self._index = datas[0]\n",
    "\n",
    "                datae = datas[1].encode('utf-8')\n",
    "                datae = re.sub('(（|\\(|\\（).+?(）|\\）|\\))','', datae) #remove brackets\n",
    "\n",
    "                #exception\n",
    "                exception = ['A・T・C事務所']\n",
    "                for e in exception:\n",
    "                    if e in datae:\n",
    "                        self._value.append(e)\n",
    "                        datae = datae.replace(e,'')\n",
    "                \n",
    "                self._value.extend(re.split(u'[,、・/／→]', datae.decode('utf-8'))) #separate multiple staffs                \n",
    "                for i, v in enumerate(self._value): #remove () and ※\n",
    "                    self._value[i]  = re.split(u'[※]', v)[0]\n",
    "#                     print '    ',  self._value[i]\n",
    "               \n",
    "                if self._index in self._staff_dict:\n",
    "                    self._staff_dict[self._index].extend(self._value)\n",
    "                else:\n",
    "                     self._staff_dict[self._index] = self._value\n",
    "                \n",
    "                self._value = []\n",
    "\n",
    "        if self._find_headline and tag==u'span':\n",
    "            self._find_headline = False\n",
    "\n",
    "    def handle_data(self, data):\n",
    "#         if self._find_staff: \n",
    "#             print data\n",
    "#             print self._li_count, self._dd_count\n",
    "        if self._find_staff and self._li_count>0 or self._dd_count>0:\n",
    "                 if not self._find_ref :\n",
    "                        self._data_temp += data\n",
    "#                 print self._data_temp\n",
    "#             if not self._find_colon:\n",
    "#                 self._find_colon = True\n",
    "#                 datas = re.split(u'[：-]', data)\n",
    "#                 self._index = datas[0]\n",
    "#                 self._value.extend(re.split(u'[,・/,／]', datas[1]))\n",
    "#             else:\n",
    "#                 self._value.extend(re.split(u'[,・/,／]', data))\n",
    "        if self._find_ref :\n",
    "            self._find_ref = False\n",
    "            \n",
    "        if self._find_headline:\n",
    "            if data == u'スタッフ':\n",
    "                self._find_staff = True\n",
    "                self._li_count = 0\n",
    "\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "parser.feed(p.html())\n",
    "\n",
    "for k, v in parser._staff_dict.items():\n",
    "    print k, ':', \n",
    "    for staff in set(v):\n",
    "        print staff,\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox = pandas.read_html(p.html(), match=u'ジャンル')\n",
    "# for i in infobox:\n",
    "#     print infobox[0].describe\n",
    "    \n",
    "ib = infobox[0]\n",
    "ib = ib.set_index(ib[0])\n",
    "ib = ib.drop(u'テンプレートを表示')\n",
    "ib = ib.drop(0, axis=1)\n",
    "# print ib.at[u'出演者',1]\n",
    "\n",
    "ib\n",
    "\n",
    "# print ib\n",
    "# ib = ib.drop('朝井リョウと加藤千恵のオールナイトニッポン0(ZERO)  ', axis=1)\n",
    "# ib.drop(u'公式サイト', axis=0)\n",
    "# print ib\n",
    "ib.loc['test'] = [ ['ddd', 'tst', 'ttt']]\n",
    "ib\n",
    "# print ib[0]\n",
    "# ib[ib[0]==u'公式サイト']\n",
    "# print ib.drop(ib[ib[0]==u'テンプレートを表示'])\n",
    "# ib.query('0=='公式サイト')\n",
    "# ib.drop(u'公式サイト', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "p = wikipedia.page('朝井リョウと加藤千恵のオールナイトニッポン0(ZERO)')\n",
    "# p = wikipedia.page('ウレロ☆シリーズ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyquery import PyQuery as pq\n",
    "query = pq(p.html(), parser='html')\n",
    "ib = query.find('.infobox')\n",
    "\n",
    "# parse infobox table from html\n",
    "staff_dict = dict()\n",
    "for tr in ib.find('tr'):\n",
    "    index = pq(tr).find('th').text()\n",
    "    value = pq(tr).find('td').text()\n",
    "    value = re.sub('(（|\\(|\\（).+?(）|\\）|\\))','', value.encode('utf-8')) #remove brackets\n",
    "    if index is not '' and v is not '':    \n",
    "        value = value.split('\\n')\n",
    "        if i in  staff_dict:\n",
    "            staff_dict[index].extend(value)\n",
    "        else:\n",
    "             staff_dict[index] = value\n",
    "\n",
    "# output\n",
    "for k, v in staff_dict.items():\n",
    "    print k, ':', \n",
    "    for staff in set(v):\n",
    "        print staff,', ',\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html as xh\n",
    "\n",
    "find_staff = False\n",
    "staff_header = ''\n",
    "staff_end = ''\n",
    "\n",
    "heads = query.find('.mw-headline')\n",
    "for h in heads:\n",
    "    if find_staff:\n",
    "        staff_end = pq(h)\n",
    "        break\n",
    "    if h.text == u'スタッフ':       \n",
    "        find_staff = True\n",
    "#         print type(h)\n",
    "#         print '&#12473;&#12479;&#12483;&#12501;'.encode('utf-8')\n",
    "#         print [h.text.encode('utf-8')]\n",
    "        print h, h.text\n",
    "        staff_header = pq(h)\n",
    "        print [staff_header.text()|]\n",
    "        staff_header = xh.tostring(h)\n",
    "        print staff_header.decode('unicode-escape')\n",
    "\n",
    "# print query.html().find(u'id=\"スタッフ\"')\n",
    "# print [query.html()]\n",
    "\n",
    "# print staff_header\n",
    "# print staff_header.find(u'スタッフ')\n",
    "\n",
    "# print type(html.encode('utf-8')), type(staff_header)\n",
    "\n",
    "# print staff_end.text()\n",
    "# print type(html.encode('utf-8'))\n",
    "# print type(staff_header.html())\n",
    "\n",
    "# uls = query.find('ul')\n",
    "# for ul in uls:\n",
    "#     lis = ul.find('li')\n",
    "#     for li in lis:\n",
    "#         print li.tex\n",
    "\n",
    "# query('li').outerHtml()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "wikipedia.set_lang('ja')\n",
    "p = wikipedia.page('朝井リョウと加藤千恵のオールナイトニッポン0(ZERO)')\n",
    "# p = wikipedia.page('ウレロ☆シリーズ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bs4.element as bs4elem\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "bsObj = BeautifulSoup(p.html(), \"html.parser\")\n",
    "\n",
    "htmls = []\n",
    "headers = bsObj.find_all(class_='mw-headline')\n",
    "\n",
    "find_staff = False\n",
    "staff_start = ''\n",
    "staff_end = ''\n",
    "for h in headers:\n",
    "#         if find_staff:\n",
    "#             staff_end_class = h\n",
    "#             break\n",
    "        if h.text == u'スタッフ':\n",
    "            staff_start = h\n",
    "#             break\n",
    "            find_staff = True\n",
    "    \n",
    "last_text = ''\n",
    "def find_list(elt, parent=''):\n",
    "    global last_text\n",
    "    for e in elt.find_all('ul'):\n",
    "        for s in e.parent.strings:\n",
    "            parent += s.strip('\\n') #repr(s).decode('unicode-escape')\n",
    "            break            \n",
    "        find_list(e, parent)\n",
    "        parent = ''\n",
    "        e.parent.decompose()\n",
    "    for e in elt.find_all(['dd', 'li']):\n",
    "        if e.name == 'dd':\n",
    "            print last_text + e.text\n",
    "            last_text = ''\n",
    "        else:\n",
    "            last_text = parent+e.text\n",
    "            print parent+e.text        \n",
    "#     \n",
    "\n",
    "finish = False\n",
    "for elt in staff_start.parent.nextSiblingGenerator():\n",
    "    for e in elt:\n",
    "        if isinstance(e, bs4elem.Tag) and e.get('class') == [u'mw-headline']:\n",
    "            finish = True\n",
    "            break\n",
    "    if finish:\n",
    "        break\n",
    "\n",
    "    if hasattr(elt, \"text\"):\n",
    "        find_list(elt)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "bsObj = BeautifulSoup(p.html(), \"html.parser\")\n",
    "\n",
    "table = bsObj.findAll(\"table\",{\"class\":\"infobox\"})[0]\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "result = dict()\n",
    "\n",
    "for row in rows:\n",
    "    index = row.find('th')\n",
    "    if index is not None:\n",
    "        index = index.get_text()\n",
    "        value = row.find('td')\n",
    "        if value is not None:\n",
    "            value = BeautifulSoup(str(value).replace('<br/>', '\\n'), \"html.parser\")\n",
    "            value = value.get_text().split('\\n')\n",
    "            for v in value:\n",
    "                print v,',      ',\n",
    "            print ''\n",
    "\n",
    "            result[index] = value\n",
    "\n",
    "# output\n",
    "# for k, v in result.items():\n",
    "#     print k, ':', \n",
    "#     for staff in set(v):\n",
    "#         print staff,', ',\n",
    "#     print ''\n",
    "\n",
    "#             for i, elm in enumerate(value.childGenerator()):\n",
    "#                 print(i, \":\", elm)\n",
    "#                 print elm.get_text()\n",
    "\n",
    "        \n",
    "#         brs = value.findAll('br')\n",
    "#         print brs\n",
    "#         value = value.get_text()\n",
    "#         if value[:1] == '\\n':\n",
    "#             value = value[1:]\n",
    "        \n",
    "#     if index is not None and value is not None:\n",
    "#         print index + u': ' + value\n",
    "#     else:\n",
    "#         print index, value\n",
    "\n",
    "\n",
    "\n",
    "#     for cell in row.findAll(['td', 'th']):\n",
    "#         csvRow.append(cell.get_text())\n",
    "        \n",
    "#     for v in csvRow:\n",
    "#         print v,  \n",
    "#     print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'\\n'.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'h3' >= 'h4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "if re.match('h[1-6]', 'h1'):\n",
    "    print 'test'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
