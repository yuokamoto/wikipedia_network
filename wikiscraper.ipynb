{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "wikipedia.set_lang('ja')\n",
    "p = None\n",
    "p = wikipedia.page('朝井リョウ&加藤千恵のオールナイトニッポン0(ZERO)',  )\n",
    "# p = wikipedia.page('ウレロ☆シリーズ')\n",
    "# p = wikipedia.page('怪奇恋愛作戦')\n",
    "# p = wikipedia.page('アルコ&ピースのオールナイトニッポンシリーズ')\n",
    "# p = wikipedia.page(pageid=129204)\n",
    "# p = wikipedia.page('水曜どうでしょう')\n",
    "# p = wikipedia.page('おぎやはぎのメガネびいき')\n",
    "# p = wikipedia.page('リーガルハイ')\n",
    "# p = wikipedia.page('リバースエッジ大川端探偵社')\n",
    "# p = wikipedia.page('SPEC〜警視庁公安部公安第五課 未詳事件特別対策係事件簿〜')\n",
    "# p = wikipedia.page('勝手にふるえてろ')\n",
    "# p = wikipedia.page('ストロベリーナイト(テレビドラマ)')\n",
    "# p = wikipedia.page('アイアンマン (映画)')\n",
    "# print p.html()\n",
    "print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import bs4.element as bs4elem\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# todo handle English+漢字, '-.,' in the name\n",
    "EXCEPTION = [u'A・T・C事務所',u'Scope co.,ltd.', u'4-Legs',  u'オア・グローリー神宮前店', u'インダストリアル・ライト&マジック']\n",
    "\n",
    "# convert unicode to utf-8 if data is unicode\n",
    "def utf(data):\n",
    "    if isinstance(data, unicode):\n",
    "        data = data.encode('utf-8')\n",
    "    return data\n",
    "\n",
    "# return true if data include only Katakana and alphabet\n",
    "def isKatakanaOrEng(data):\n",
    "    for d in data:\n",
    "        if not ( '゠' <= utf(d) <= 'ヿ' or re.search('\\w', utf(d)) ):\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "class WikiScraper(object):\n",
    "    def __init__(self):\n",
    "        self._name = ''\n",
    "        self._page = ''\n",
    "        self._bsObj = None\n",
    "        self._headlines = []\n",
    "        self._headline_tag = 'h3'\n",
    "        self._last_key = ''\n",
    "        self._result = dict()\n",
    "\n",
    "    # return string of 'key : value \\n'\n",
    "    def __str__(self):\n",
    "        output = self._name + '\\n'\n",
    "        for k, v in self._result.items():\n",
    "            output +=  k + ' : ' \n",
    "            for staff in set(v):\n",
    "                output += staff + ',       '\n",
    "            output += u'\\n'\n",
    "        return output.encode('utf-8')\n",
    "    \n",
    "    # load html data from wikipedia\n",
    "    # name: title of wikipage\n",
    "    # lang: wikipedia language\n",
    "    # pageid : id of wikipage.\n",
    "    def load_wiki(self, name, lang='ja', pageid=np.nan):\n",
    "        self._name = name\n",
    "        wikipedia.set_lang(lang)\n",
    "        try:\n",
    "            if np.isnan(pageid):\n",
    "                self._page = wikipedia.page(self._name)\n",
    "            else:\n",
    "                self._page = wikipedia.page(pageid=pageid)\n",
    "            self._bsObj = BeautifulSoup(self._page.html(), \"html.parser\")   \n",
    "        except:\n",
    "            print('Can not open wikipage of '+self._name+' with lang '+lang)    \n",
    "    \n",
    "    # load html\n",
    "    # name: title of wikipage\n",
    "    # html: html string\n",
    "    def load_html(self, name, html):\n",
    "        self._bsObj = BeautifulSoup(html, \"html.parser\")   \n",
    "        self._name = name\n",
    "    \n",
    "    # find headline\n",
    "    # name name of headline, e.g. スタッフ\n",
    "    def find_headline(self, name):\n",
    "        headers = self._bsObj.find_all(class_='mw-headline')\n",
    "        self._headlines = []\n",
    "        for h in headers:\n",
    "                if re.search(name, h.text):\n",
    "                    self._headline_tag = h.parent.name\n",
    "                    self._headlines.append(h)\n",
    "\n",
    "    # cleauup text\n",
    "    # wrapper function\n",
    "    # remove brackets, blank, and \\n\n",
    "    def cleanup_text(func):\n",
    "        def wrapper(self, key, data):\n",
    "            value = []\n",
    "            data = data.encode('utf-8')\n",
    "            data = re.sub('(［|\\[).+?(］|\\])','', data) #remove brackets\n",
    "            data = re.sub('(（|\\(|\\（).+?(）|\\）|\\))','', data) #remove brackets\n",
    "            data = data.strip()  #remove \\n, and blank from head and tail\n",
    "            \n",
    "            #exception\n",
    "            for e in EXCEPTION:\n",
    "                if e.encode('utf-8') in data:\n",
    "                    value.append(e)\n",
    "                    data = data.replace(e.encode('utf-8'),'')\n",
    "            \n",
    "            value = func(key, data, value)\n",
    "            key = key.strip()\n",
    "            if key in self._result:\n",
    "                self._result[key].extend(value)\n",
    "            else:\n",
    "                 self._result[key] = value\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    # cleanup value\n",
    "    # separate line with , / -> and etc. \n",
    "    @cleanup_text\n",
    "    def cleanup_value_list(key, data, value):\n",
    "        data = re.split('-', data)[0]\n",
    "        \n",
    "        data = re.split(u'[,、/／→\\n]', data.decode('utf-8')) #separate multiple staffs    \n",
    "        for d in data:\n",
    "            d = d.strip()\n",
    "            if not isKatakanaOrEng(d): #separate with '・' for ウレロシリーズ\n",
    "                value.extend(re.split(u'[・]', d)) #separate multiple staffs    \n",
    "#             value.extend(re.split(u'[,、/／→\\n]', data.decode('utf-8'))) #separate multiple staffs    \n",
    "            else:\n",
    "                value.append(d) # do not separate with '・' for foregin people\n",
    "#             value.extend(re.split(u'[,、/・／→\\n]', data.decode('utf-8'))) #separate multiple staffs    \n",
    "\n",
    "        for i, v in enumerate(value): #remove ※\n",
    "            value[i]  = re.split(u'※|●', v)[0].strip()\n",
    "        \n",
    "        return value\n",
    "\n",
    "    # cleanup value\n",
    "    # separate line with , / -> and etc. \n",
    "    @cleanup_text\n",
    "    def cleanup_value_infobox(key, data, value): \n",
    "        data = re.split(u'[※|●]', data)[0]\n",
    "        data = re.split(u'[,、/→\\n]', data.decode('utf-8')) #separate multiple staffs    \n",
    "        for d in data:\n",
    "            d = d.strip()\n",
    "            if not isKatakanaOrEng(d): #separate with '・' for ウレロシリーズ\n",
    "                value.extend(re.split(u'[・]', d)) #separate multiple staffs    \n",
    "            else:\n",
    "                value.append(d) # do not separate with '・' for foregin people\n",
    "\n",
    "        return value\n",
    "    \n",
    "    # get list data\n",
    "    def find_list(self, elt, parent=''):\n",
    "        # recursive process if there are dl or ul\n",
    "        for e in elt.find_all(['dl', 'ul']):\n",
    "#             print e.parent.text\n",
    "            if e.parent is not None:\n",
    "                temp =  e.parent.text.split('\\n')[0]\n",
    "                if temp:\n",
    "                    # researve text as parents\n",
    "                    parent += e.parent.text.split('\\n')[0] + ' '\n",
    "                    self._last_key = parent\n",
    "#             for s in e.parent.strings:\n",
    "#                 parent += s.strip() + ' '\n",
    "#                 self._last_key = parent\n",
    "# #                 print parent\n",
    "#                 break            \n",
    "            self.find_list(e, parent)\n",
    "            parent = ''\n",
    "            if e.parent is not None:\n",
    "                e.parent.decompose()\n",
    "        \n",
    "        # get data from each dd, li or dt\n",
    "        for e in elt.find_all(['dd', 'li', 'dt']):\n",
    "            data = e.text\n",
    "            \n",
    "            if e.name == 'dd':\n",
    "                # add data if there is data already ref:朝井リョウ・加藤千恵のオールナイトニッポン staff\n",
    "                if self._result.get(self._last_key) is not None:\n",
    "                    data = self._result[self._last_key][-1]+ data\n",
    "                key = self._last_key\n",
    "                self._last_value = ''\n",
    "\n",
    "            elif e.name == 'dt':\n",
    "                # researve data as a parent\n",
    "                parent = data\n",
    "                self._last_key = parent\n",
    "                continue\n",
    "                \n",
    "            elif not re.search(u'[:：-＝\\-\\=]',data): \n",
    "                # if data do not have :, - and etc, i.e. the separator of role and people, save data as a parent\n",
    "                if parent == '':\n",
    "                    parent = self._last_key\n",
    "                key = parent.strip()\n",
    "                self._last_value = ''\n",
    "\n",
    "            else:\n",
    "                #temp for 水曜どうでしょう　& アルピーANN\n",
    "                if parent != '':\n",
    "                    # if parents is not empty, set parent as key\n",
    "                    key = parent\n",
    "                    if re.search(u'：',data):\n",
    "                        #if ':' is in data, split and data[0] is key and others will be data\n",
    "                        datas = re.split(u'：', data)\n",
    "                        key += datas[0]\n",
    "                        \n",
    "                        # connect data except for data[0] in case data has multiple ':'\n",
    "                        data_temp = ''\n",
    "                        for d in datas[1:]:\n",
    "                            data_temp += d\n",
    "                        data = data_temp\n",
    "                    \n",
    "                    elif re.search(u'-',data):\n",
    "                        # separate data with '-'  and set first one as data, e.g. アルコ&ピースのANN\n",
    "                        data = re.split(u'-', data)\n",
    "                        key = parent\n",
    "                        data = data[0]\n",
    "                        \n",
    "                else:\n",
    "                    if re.search(u'[:：-＝\\-\\=]',data): \n",
    "                        # if parent is empty, separate data and set [0] as key and others as data\n",
    "                        datas = re.split(u'[:：-＝\\-\\=]', data)\n",
    "                        key = datas[0].strip() \n",
    "                        \n",
    "                        data_temp = ''\n",
    "                        for d in datas[1:]:\n",
    "                            data_temp += d\n",
    "                        data = data_temp\n",
    "\n",
    "                    else :\n",
    "                        #if parent is empty and no separator, use last key and\n",
    "                        key = self._last_key\n",
    "            \n",
    "            value = self.cleanup_value_list(key, data)\n",
    "\n",
    "            self._last_key = key\n",
    "                \n",
    "    def get_list_from_headline(self, name):\n",
    "        self.find_headline(name)\n",
    "        for headline_start in self._headlines:\n",
    "            finish = False\n",
    "            # get html between finded headline and next h[1-6]\n",
    "            for elt in headline_start.parent.nextSiblingGenerator():\n",
    "                if elt.name is not None and re.match('h[1-6]', elt.name) and elt.name <= self._headline_tag:\n",
    "                    break\n",
    "\n",
    "                if hasattr(elt, 'text'):\n",
    "                    self.find_list(elt)\n",
    "\n",
    "    def find_tr(self, table):\n",
    "        rows = table.findAll('tr')\n",
    "        for row in rows:\n",
    "            if len(row.findAll('tr')):\n",
    "                self.find_tr(row)\n",
    "            else:\n",
    "                index = row.find('th')\n",
    "                if index is not None:\n",
    "                    key = index.get_text()\n",
    "                    value = row.find('td')\n",
    "                    if value is not None:\n",
    "                        # replace <br> with \\n  to get by .text()\n",
    "                        value_str = re.sub('</*br/*>', '\\n', str(value))\n",
    "                        value = BeautifulSoup(value_str, \"html.parser\")\n",
    "        #                    value = value.get_text().split('\\n')\n",
    "                        value = self.cleanup_value_infobox(key, value.get_text())\n",
    "        #                     self._result[key] = value\n",
    "\n",
    "\n",
    "    def get_table(self, class_name='infobox'):\n",
    "        tables = self._bsObj.findAll('table',{'class':class_name})\n",
    "        for table in tables:\n",
    "            self.find_tr(table)\n",
    "                    \n",
    "wsc = WikiScraper()\n",
    "#wsc.load_wiki('朝井リョウと加藤千恵のオールナイトニッポン0(ZERO)', 'ja')\n",
    "wsc.load_html(p.title, p.html())\n",
    "# wsc.load_wiki(u'攻殻機動隊', lang='ja', pageid=129204)\n",
    "wsc.get_list_from_headline(u'.*スタッフ')\n",
    "wsc.get_table('infobox')\n",
    "# print '-----------------------------'\n",
    "print wsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"contents.csv\",index_col=0)\n",
    "wscs = []\n",
    "for i in data.index.values:\n",
    "    wsc = WikiScraper()\n",
    "    wsc.load_wiki(i.decode('utf-8'), pageid=data.at[i, 'id'], lang='ja')\n",
    "#         wsc.load_wiki(i.decode('utf-8'), 'ja')\n",
    "# #     wsc.load_html(p.html())\n",
    "    if wsc._bsObj is not None:\n",
    "        wsc.get_list_from_headline(u'.*スタッフ')\n",
    "        wsc.get_table('infobox')\n",
    "    \n",
    "    wscs.append(wsc)\n",
    "    \n",
    "    print i\n",
    "    print '-------------------------------------'\n",
    "    print wsc\n",
    "    print '+++++++++++++++++++++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "max_number_of_nodes = 500\n",
    "count_once = True\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "key_rm = [u'放送期間', u'放送時間', u'公開', u'上映時間', u'次作', u'回数', u'放送分']\n",
    "value_rm = [u'', u'同上', u'日本', u'日本語', u'英語', u'公式サイト', u'ほか', u'ステレオ放送', u'文字多重放送',u'歴代エンディングテーマを参照',\n",
    "            u'フジテレビ番組基本情報']\n",
    "\n",
    "for wsc in wscs:\n",
    "    \n",
    "    point = float(data.at[wsc._name.encode('utf-8'), 'ポイント'])\n",
    "    G.add_node(wsc._name, genre='content', point=point)\n",
    "    attrs = set()\n",
    "    for k, v in wsc._result.items():\n",
    "        #add first genre to node attribute\n",
    "        if k == u'ジャンル':\n",
    "            if len(v)==0:\n",
    "                G.node[wsc._name]['genre'] = v\n",
    "            else:\n",
    "                G.node[wsc._name]['genre'] = v[0]\n",
    "        elif k not in key_rm:\n",
    "            for attr in v:\n",
    "                if attr not in data.index.values and attr not in value_rm: #do nothing for contents itself\n",
    "                    if G.node.get(attr) is None: #first time\n",
    "                        G.add_node(attr, genre='attribute', point=point)\n",
    "                    elif not (attr in attrs and count_once): # from second time\n",
    "                        G.node[attr]['point'] += point\n",
    "                    G.add_edge(wsc._name, attr, relation=k) \n",
    "                    attrs.add(attr)\n",
    "    print wsc._name, len(G.nodes)\n",
    "        \n",
    "cliques = nx.find_cliques(G)\n",
    "# #標準出力\n",
    "# for c in cliques:\n",
    "#     print c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nodes for visualization\n",
    "first = True\n",
    "while len(G.nodes)>max_number_of_nodes or first:\n",
    "    print 'Threshold: ', threshold\n",
    "    print ' node num before simplify:', len(G.nodes)\n",
    "    value_rm.extend([node for node,degree in dict(G.degree()).items() if degree < 2 and G.node[node]['genre'] is 'attribute']) #remove node based on degree\n",
    "    value_rm.extend([node for node in G.nodes if  G.node[node]['point'] < threshold and G.node[node]['genre'] is 'attribute'])\n",
    "    G.remove_nodes_from(value_rm)\n",
    "    print ' node num after simplify:', len(G.nodes)\n",
    "    threshold += 0.1\n",
    "    first = False\n",
    "\n",
    "#mix the closed genres\n",
    "for v in data.index.values:\n",
    "    title = v.decode('utf-8')\n",
    "    if re.search( u'(.*バラエティ.*)|(.*お笑い.*)', G.node[title]['genre']):\n",
    "        G.node[title]['genre'] = u'バラエティ'\n",
    "    if re.search( u'.*ドラマ.*', G.node[title]['genre']):\n",
    "        G.node[title]['genre'] = u'ドラマ'\n",
    "    if re.search( u'.*SF.*', G.node[title]['genre']):\n",
    "        G.node[title]['genre'] = u'SF'\n",
    "\n",
    "nx.draw_networkx(G,font_family='AppleGothic',font_size =8)\n",
    "# plt.show()\n",
    "nx.write_gexf(G, 'result.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"500px\"\n",
       "            src=\"result.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11533fa50>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#option1\n",
    "#visualize with pyvis + networkx \n",
    "from pyvis.network import Network\n",
    "# nxg = nx.complete_graph(10)\n",
    "# gg = Network(notebook=True)\n",
    "# gg.from_nx(G)\n",
    "# gg.show(\"nx.html\")\n",
    "\n",
    "def get_net_node(net, key, value):\n",
    "    for item in net.nodes:\n",
    "        if item[key] == value:\n",
    "            return item\n",
    "    return None \n",
    "\n",
    "net = Network(height='500px', width='1000px', notebook=True)\n",
    "# net = Network(height='500px', width='500px', notebook=False)\n",
    "net.force_atlas_2based(\n",
    "    gravity=-30, \n",
    "    central_gravity=0.01, \n",
    "    spring_length=0.1, \n",
    "    spring_strength=0.1, \n",
    "    damping=1.0, \n",
    "    overlap=1.0)\n",
    "\n",
    "node_id = 0\n",
    "for label in G.nodes:\n",
    "#     print label, G.node[label]['genre'], G.node[label]['point']\n",
    "    if G.node[label]['genre'] == 'attribute':\n",
    "        shape = 'dot'\n",
    "    else:\n",
    "        shape = 'box'\n",
    "    net.add_node(node_id, label=label, \n",
    "                 group=G.node[label]['genre'], \n",
    "                 value=G.node[label]['point']*1.0,\n",
    "                 mass=G.node[label]['point'],\n",
    "                shape=shape)\n",
    "    node_id += 1\n",
    "    \n",
    "for edge in G.edges:\n",
    "    edge[0], edge[1]\n",
    "    net.add_edge( get_net_node(net, 'label', edge[0])['id'], \n",
    "                 get_net_node(net, 'label', edge[1])['id'])\n",
    "    \n",
    "# net.save_graph(\"result.html\")\n",
    "net.show(\"result.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
